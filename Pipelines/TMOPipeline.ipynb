{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "SZXN0o2DXDOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c49dae-a53a-49f4-ca2b-63568ccb1894"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Yk1EVZp7Vx1-"
      },
      "outputs": [],
      "source": [
        "# -------------------- IGNORE THIS BLOCK AT THE MOMENT --------------------\n",
        "\n",
        "\n",
        "# Assuming G, D, and F are your generator, discriminator, and detector models\n",
        "\n",
        "# Detector loss components (simplified)\n",
        "class DetectorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DetectorLoss, self).__init__()\n",
        "        # Define loss components, e.g., Focal Loss for classification\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # Compute classification and localization loss\n",
        "        # Lcls and Lloc from Eq. (5)\n",
        "        return lcls + lloc\n",
        "\n",
        "# Generator and Discriminator loss components\n",
        "class GANDetLoss(nn.Module):\n",
        "    def __init__(self, alpha_det=1.0, alpha_non_det=1.0, beta=0.8, gamma=10.0):\n",
        "        super(GANDetLoss, self).__init__()\n",
        "        self.alpha_det = alpha_det\n",
        "        self.alpha_non_det = alpha_non_det\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        # Additional components, e.g., feature matching loss, could be defined here\n",
        "\n",
        "    def forward(self, hdr_images, ldr_images, ground_truth):\n",
        "        # Calculate LG, LD, and LDet as described in Eq. (4), (6), and (5)\n",
        "        # This includes calling the DetectorLoss for LDet\n",
        "        # Note: This is a simplified outline. Actual implementation will depend on how G, D, and F are defined\n",
        "        return ltmo_det\n",
        "\n",
        "# Instantiate the loss\n",
        "loss_fn = GANDetLoss()\n",
        "\n",
        "# Example forward pass (simplified)\n",
        "# hdr_images, ldr_images, ground_truth = your data loading logic here\n",
        "\n",
        "# loss = loss_fn(hdr_images, ldr_images, ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- IGNORE THIS BLOCK AT THE MOMENT --------------------\n",
        "\n",
        "def discriminator_loss(D, real_images, fake_images):\n",
        "  real_loss = F.mse_loss(D(real_images), torch.ones_like(D(real_images)))\n",
        "  fake_loss = F.mse_loss(D(fake_images), torch.zeros_like(D(fake_images)))\n",
        "  return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(G, fake_images):\n",
        "  return F.mse_loss(G(fake_images), torch.ones_like(G(fake_images)))\n",
        "\n",
        "# Optional: L1 Content Loss to enforce similarity between generated and real LDR images\n",
        "def l1_content_loss(fake_images, real_images):\n",
        "    return torch.mean(torch.abs(fake_images - real_images))\n",
        "\n",
        "\n",
        "# def detector_loss():\n",
        "#   # TBD\n",
        "#   pass"
      ],
      "metadata": {
        "id": "Hu82CHj2WGmM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- BASIC GAN LOSSES -------------------------\n",
        "\n",
        "def generator_adversarial_loss(D, fake_images):\n",
        "    return torch.mean((D(fake_images) - 1) ** 2)\n",
        "\n",
        "def discriminator_loss(D, real_images, fake_images):\n",
        "    real_loss = torch.mean((D(real_images) - 1) ** 2)\n",
        "    fake_loss = torch.mean(D(fake_images) ** 2)\n",
        "    return (real_loss + fake_loss) / 2"
      ],
      "metadata": {
        "id": "iXqnnyIMXTWU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- GENERATOR ARCHITECTURE -------------------------\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.ins = nn.InstanceNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.conv(x)\n",
        "      x = self.ins(x)\n",
        "      x = self.relu(x)\n",
        "      return x\n",
        "\n",
        "class AttentionModule(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(AttentionModule, self).__init__()\n",
        "    self.attention_score = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "      score = self.attention_score(x)\n",
        "      score = self.sigmoid(score)\n",
        "      attention_map = torch.mul(score, x)\n",
        "      return attention_map\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.conv1 = ConvBlock(in_channels, 32, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv2 = ConvBlock(32, 64, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv3 = ConvBlock(64, 128, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv4 = ConvBlock(512, 128, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv5 = ConvBlock(128, 64, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv6 = ConvBlock(64, 3, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.attention1 = AttentionModule(32, 32)\n",
        "        self.attention2 = AttentionModule(64, 64)\n",
        "        self.attention3 = AttentionModule(512, 512)\n",
        "\n",
        "        self.k3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.k5 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.k7 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3)\n",
        "        self.k9 = nn.Conv2d(128, 128, kernel_size=9, stride=1, padding=4)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest') # TESTING\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        att_1 = self.attention1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool(x)\n",
        "        att_2 = self.attention2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool(x)\n",
        "        x_1 = self.k3(x)\n",
        "        x_2 = self.k5(x)\n",
        "        x_3 = self.k7(x)\n",
        "        x_4 = self.k9(x)\n",
        "        x = torch.cat((x_1, x_2, x_3, x_4), dim=1) # TESTING\n",
        "        att_3 = self.attention3(x)\n",
        "        x = self.upsample(att_3) # CONTINUE CODING HERE\n",
        "        x = self.conv4(x)\n",
        "        x = self.upsample(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.upsample(x)\n",
        "        x = self.conv6(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "6fq4dVtTiD6G"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- DISCRIMINATOR ARCHITECTURE -------------------------\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        # Block 1 Input -> Channels x H x W\n",
        "        nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Block 2 Input -> 64 * H/2 * W/2\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(128),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
        "        nn.InstanceNorm2d(512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.model(x)"
      ],
      "metadata": {
        "id": "HVTIYlw2MXpW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- PATCH DISCRIMINATOR -------------------\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(PatchDiscriminator, self).__init__()\n",
        "\n",
        "        # Define layers\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "\n",
        "        # Instance normalization\n",
        "        self.instance_norm = nn.InstanceNorm2d(512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers\n",
        "        x = nn.LeakyReLU(0.2)(self.conv1(x))\n",
        "        x = nn.LeakyReLU(0.2)(self.instance_norm(self.conv2(x)))\n",
        "        x = nn.LeakyReLU(0.2)(self.instance_norm(self.conv3(x)))\n",
        "        x = nn.LeakyReLU(0.2)(self.instance_norm(self.conv4(x)))\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "slrlvWvZ8AmB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H = 256\n",
        "W = 256\n",
        "# batch_size = 1\n",
        "in_channels =3\n",
        "# Testing\n",
        "\n",
        "gen_model = Generator(in_channels=3)\n",
        "print(summary(gen_model, input_size=(batch_size, in_channels, H, W)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4YyqCw3x29C",
        "outputId": "49032dcd-f3fa-492f-97e9-0bfafe6655c6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HELLO\n",
            "torch.Size([1, 32, 256, 256])\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "Generator                                [1, 3, 256, 256]          --\n",
            "├─ConvBlock: 1-1                         [1, 32, 256, 256]         --\n",
            "│    └─Conv2d: 2-1                       [1, 32, 256, 256]         128\n",
            "│    └─InstanceNorm2d: 2-2               [1, 32, 256, 256]         --\n",
            "│    └─ReLU: 2-3                         [1, 32, 256, 256]         --\n",
            "├─MaxPool2d: 1-2                         [1, 32, 128, 128]         --\n",
            "├─AttentionModule: 1-3                   [1, 32, 128, 128]         --\n",
            "│    └─Conv2d: 2-4                       [1, 32, 128, 128]         1,056\n",
            "│    └─Sigmoid: 2-5                      [1, 32, 128, 128]         --\n",
            "├─ConvBlock: 1-4                         [1, 64, 128, 128]         --\n",
            "│    └─Conv2d: 2-6                       [1, 64, 128, 128]         2,112\n",
            "│    └─InstanceNorm2d: 2-7               [1, 64, 128, 128]         --\n",
            "│    └─ReLU: 2-8                         [1, 64, 128, 128]         --\n",
            "├─MaxPool2d: 1-5                         [1, 64, 64, 64]           --\n",
            "├─AttentionModule: 1-6                   [1, 64, 64, 64]           --\n",
            "│    └─Conv2d: 2-9                       [1, 64, 64, 64]           4,160\n",
            "│    └─Sigmoid: 2-10                     [1, 64, 64, 64]           --\n",
            "├─ConvBlock: 1-7                         [1, 128, 64, 64]          --\n",
            "│    └─Conv2d: 2-11                      [1, 128, 64, 64]          8,320\n",
            "│    └─InstanceNorm2d: 2-12              [1, 128, 64, 64]          --\n",
            "│    └─ReLU: 2-13                        [1, 128, 64, 64]          --\n",
            "├─MaxPool2d: 1-8                         [1, 128, 32, 32]          --\n",
            "├─Conv2d: 1-9                            [1, 128, 32, 32]          147,584\n",
            "├─Conv2d: 1-10                           [1, 128, 32, 32]          409,728\n",
            "├─Conv2d: 1-11                           [1, 128, 32, 32]          802,944\n",
            "├─Conv2d: 1-12                           [1, 128, 32, 32]          1,327,232\n",
            "├─AttentionModule: 1-13                  [1, 512, 32, 32]          --\n",
            "│    └─Conv2d: 2-14                      [1, 512, 32, 32]          262,656\n",
            "│    └─Sigmoid: 2-15                     [1, 512, 32, 32]          --\n",
            "├─Upsample: 1-14                         [1, 512, 64, 64]          --\n",
            "├─ConvBlock: 1-15                        [1, 128, 64, 64]          --\n",
            "│    └─Conv2d: 2-16                      [1, 128, 64, 64]          65,664\n",
            "│    └─InstanceNorm2d: 2-17              [1, 128, 64, 64]          --\n",
            "│    └─ReLU: 2-18                        [1, 128, 64, 64]          --\n",
            "├─Upsample: 1-16                         [1, 128, 128, 128]        --\n",
            "├─ConvBlock: 1-17                        [1, 64, 128, 128]         --\n",
            "│    └─Conv2d: 2-19                      [1, 64, 128, 128]         8,256\n",
            "│    └─InstanceNorm2d: 2-20              [1, 64, 128, 128]         --\n",
            "│    └─ReLU: 2-21                        [1, 64, 128, 128]         --\n",
            "├─Upsample: 1-18                         [1, 64, 256, 256]         --\n",
            "├─ConvBlock: 1-19                        [1, 3, 256, 256]          --\n",
            "│    └─Conv2d: 2-22                      [1, 3, 256, 256]          195\n",
            "│    └─InstanceNorm2d: 2-23              [1, 3, 256, 256]          --\n",
            "│    └─ReLU: 2-24                        [1, 3, 256, 256]          --\n",
            "==========================================================================================\n",
            "Total params: 3,040,035\n",
            "Trainable params: 3,040,035\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 3.55\n",
            "==========================================================================================\n",
            "Input size (MB): 0.79\n",
            "Forward/backward pass size (MB): 58.20\n",
            "Params size (MB): 12.16\n",
            "Estimated Total Size (MB): 71.14\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patch_disc_model = PatchDiscriminator(in_channels=3)\n",
        "print(summary(patch_disc_model, input_size=(batch_size, in_channels, H, W)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk7dMYGl8twy",
        "outputId": "876697dd-b5c9-466e-d545-3f78edbebd64"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "PatchDiscriminator                       [1, 1, 30, 30]            --\n",
            "├─Conv2d: 1-1                            [1, 64, 128, 128]         3,136\n",
            "├─Conv2d: 1-2                            [1, 128, 64, 64]          131,200\n",
            "├─InstanceNorm2d: 1-3                    [1, 128, 64, 64]          --\n",
            "├─Conv2d: 1-4                            [1, 256, 32, 32]          524,544\n",
            "├─InstanceNorm2d: 1-5                    [1, 256, 32, 32]          --\n",
            "├─Conv2d: 1-6                            [1, 512, 31, 31]          2,097,664\n",
            "├─InstanceNorm2d: 1-7                    [1, 512, 31, 31]          --\n",
            "├─Conv2d: 1-8                            [1, 1, 30, 30]            8,193\n",
            "==========================================================================================\n",
            "Total params: 2,764,737\n",
            "Trainable params: 2,764,737\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 3.15\n",
            "==========================================================================================\n",
            "Input size (MB): 0.79\n",
            "Forward/backward pass size (MB): 18.62\n",
            "Params size (MB): 11.06\n",
            "Estimated Total Size (MB): 30.47\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disc_model = Discriminator(in_channels=3)\n",
        "print(summary(disc_model, input_size=(batch_size, in_channels, H, W)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq0Sv6g7y_6g",
        "outputId": "3cf27ceb-40a7-4380-c468-bbade5dcb829"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "Discriminator                            [1, 1, 30, 30]            --\n",
            "├─Sequential: 1-1                        [1, 1, 30, 30]            --\n",
            "│    └─Conv2d: 2-1                       [1, 64, 128, 128]         3,136\n",
            "│    └─LeakyReLU: 2-2                    [1, 64, 128, 128]         --\n",
            "│    └─Conv2d: 2-3                       [1, 128, 64, 64]          131,200\n",
            "│    └─InstanceNorm2d: 2-4               [1, 128, 64, 64]          --\n",
            "│    └─LeakyReLU: 2-5                    [1, 128, 64, 64]          --\n",
            "│    └─Conv2d: 2-6                       [1, 256, 32, 32]          524,544\n",
            "│    └─InstanceNorm2d: 2-7               [1, 256, 32, 32]          --\n",
            "│    └─LeakyReLU: 2-8                    [1, 256, 32, 32]          --\n",
            "│    └─Conv2d: 2-9                       [1, 512, 31, 31]          2,097,664\n",
            "│    └─InstanceNorm2d: 2-10              [1, 512, 31, 31]          --\n",
            "│    └─LeakyReLU: 2-11                   [1, 512, 31, 31]          --\n",
            "│    └─Conv2d: 2-12                      [1, 1, 30, 30]            8,193\n",
            "==========================================================================================\n",
            "Total params: 2,764,737\n",
            "Trainable params: 2,764,737\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 3.15\n",
            "==========================================================================================\n",
            "Input size (MB): 0.79\n",
            "Forward/backward pass size (MB): 18.62\n",
            "Params size (MB): 11.06\n",
            "Estimated Total Size (MB): 30.47\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ TRAINING -------------------\n",
        "\n",
        "# Instantiate Generator and Discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Define loss function and optimizers\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (real_images, _) in enumerate(train_loader):\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # Train Discriminator\n",
        "        discriminator_optimizer.zero_grad()\n",
        "\n",
        "        # Train with real images\n",
        "        real_labels = torch.ones(batch_size, 1)\n",
        "        output = discriminator(real_images)\n",
        "        real_loss = criterion(output, real_labels)\n",
        "\n",
        "        # Train with fake images\n",
        "        fake_labels = torch.zeros(batch_size, 1)\n",
        "        # noise = torch.randn(batch_size, input_dim, 1, 1)\n",
        "        fake_images = generator(hdr_images) # NEED HDR IMAGES\n",
        "        output = discriminator(fake_images.detach())\n",
        "        fake_loss = criterion(output, fake_labels)\n",
        "\n",
        "        discriminator_loss = real_loss + fake_loss\n",
        "        discriminator_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        generator_optimizer.zero_grad()\n",
        "\n",
        "        # noise = torch.randn(batch_size, noise_dim, 1, 1)\n",
        "        fake_images = generator(hdr_images) # NEED HDR IMAGES\n",
        "        output = discriminator(fake_images)\n",
        "        generator_loss = criterion(output, real_labels)\n",
        "\n",
        "        generator_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        # Print training losses\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], \"\n",
        "                  f\"Discriminator Loss: {discriminator_loss.item():.4f}, \"\n",
        "                  f\"Generator Loss: {generator_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "72JUp_D98_zK"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}