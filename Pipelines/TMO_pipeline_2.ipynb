{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn\n",
    "from torchvision.transforms.functional import normalize as F_normalize\n",
    "from torchvision import transforms\n",
    "from torch.nn.functional import interpolate as F_upsample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import numpy as np\n",
    "import OpenEXR, Imath\n",
    "import functools\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "num_classes=7 #(6+1 for background)\n",
    "dataset_size=1270\n",
    "training_losses={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_exr(filename):\n",
    "    \"\"\"Load an EXR file and return as a NumPy array.\"\"\"\n",
    "    file = OpenEXR.InputFile(filename)\n",
    "    dw = file.header()['dataWindow']\n",
    "    size = (dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1)\n",
    "\n",
    "    pt = Imath.PixelType(Imath.PixelType.FLOAT)\n",
    "    channels = ['R', 'G', 'B']\n",
    "\n",
    "    rgb = [np.frombuffer(file.channel(c, pt), dtype=np.float32) for c in channels]\n",
    "    rgb = [np.reshape(c, (size[1], size[0])) for c in rgb]\n",
    "    \n",
    "    image = np.stack(rgb, axis=-1)\n",
    "    # image = image.resize((1080,1920), Image.LANCZOS)\n",
    "    return image\n",
    "\n",
    "class HDRDataset(Dataset):\n",
    "    def __init__(self, png_dir,exr_dir,txt_dir):\n",
    "\n",
    "        self.png_dir = png_dir\n",
    "        self.exr_dir = exr_dir\n",
    "        self.txt_dir = txt_dir\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(png_dir) if f.endswith('.png')]\n",
    "\n",
    "        self.labels = []\n",
    "        self.bboxes = []\n",
    "        self.data=[]\n",
    "        for filename in self.filenames:\n",
    "            txt_filename = filename + '.txt'\n",
    "            txt_path = os.path.join(txt_dir, txt_filename)\n",
    "            with open(txt_path, 'r') as file:\n",
    "                objects = []\n",
    "                for line in file:\n",
    "                    cls, x, y, w, h = map(float,line.strip().split()) \n",
    "                    \n",
    "                    objects.append((int(cls), [x, y, w, h]))\n",
    "                self.data.append(objects)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(self.filenames[idx])\n",
    "        png_path = os.path.join(self.png_dir, self.filenames[idx]+'.png')\n",
    "        exr_path = os.path.join(self.exr_dir, self.filenames[idx]+'.exr')\n",
    "        \n",
    "        exr_image = load_exr(exr_path)\n",
    "        exr_image=torch.tensor(exr_image).permute(2,0,1)\n",
    "    \n",
    "        png_image = Image.open(png_path)\n",
    "        png_image = np.array(png_image)\n",
    "       \n",
    "    \n",
    "        labels = []\n",
    "        bboxes = []\n",
    "        for obj in self.data[idx]:\n",
    "            labels.append(obj[0])\n",
    "            bboxes.append(obj[1])\n",
    "        \n",
    "            \n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "        \n",
    "      \n",
    "        transform_img=transforms.Resize((1080,1920))\n",
    "        \n",
    "        # Convert image and bbox to tensor\n",
    "        png_image = torch.tensor(png_image, dtype=torch.float32).permute(2, 0, 1)  # Channel first format\n",
    "        png_image=transform_img(png_image)/255.0\n",
    "        # print('png_image:',png_image)\n",
    "        exr_image=transform_img(exr_image)\n",
    "        # print(png_image.shape,exr_image.shape,len(labels),len(bboxes))\n",
    "        return exr_image,png_image, labels, bboxes\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     exr_image,png_image, labels, boxes = zip(*batch)\n",
    "#     png_image = torch.stack(png_image, dim=0)  # Stack images as they should have the same size\n",
    "#     exr_image = torch.stack(exr_image, dim=0)\n",
    "#     boxes = torch.stack(boxes, dim=0)\n",
    "#     labels=torch.stack(labels, dim=0)\n",
    "    \n",
    "#     return exr_image,png_image, labels, boxes\n",
    "train_data = HDRDataset('/home/ee/HDRDataset/train/images/train_ldr','/home/ee/HDRDataset/train/hdr_images','/home/ee/HDRDataset/train/labels/train_ldr')\n",
    "test_data = HDRDataset('/home/ee/HDRDataset/test/images/test_ldr','/home/ee/HDRDataset/test/hdr_images','/home/ee/HDRDataset/test/labels/test_ldr')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ee/Thesis/traffic_pred/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7.8125e-04, 1.3889e-03, 1.6563e-01, 1.0000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[3].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    def __init__(self, val_channels=3, key_channels=4, in_channels=0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels if in_channels else val_channels\n",
    "        self.key_channels = key_channels\n",
    "        self.val_channels = val_channels\n",
    "\n",
    "        self.keys = nn.Conv2d(self.val_channels, self.key_channels, 1)\n",
    "        self.values = nn.Conv2d(self.val_channels, self.key_channels, 1)\n",
    "        self.queries = nn.Conv2d(self.in_channels, self.key_channels, 1)\n",
    "        self.reprojection = nn.Conv2d(self.key_channels, self.val_channels, 1)\n",
    "\n",
    "    def forward(self, value_, input_=None):\n",
    "        n, c, h, w = value_.size()\n",
    "        values = self.values(value_).reshape((n, self.key_channels, h * w))\n",
    "        keys = self.keys(value_).reshape((n, self.key_channels, h * w))\n",
    "        \n",
    "        if input_ is not None:\n",
    "            queries = self.queries(input_)\n",
    "            \n",
    "            # maxpool the query if it is larger than the value \n",
    "            _, _, h_i, w_i = input_.size()\n",
    "            if w_i > w or h_i > h:\n",
    "                queries = F.max_pool2d(queries, (h_i//h, w_i//w))\n",
    "            \n",
    "            queries = queries.reshape(n, self.key_channels, h * w)\n",
    "        else:\n",
    "            queries = self.queries(value_).reshape(n, self.key_channels, h * w)\n",
    "\n",
    "        key = F.softmax(keys, dim=2)\n",
    "        query = F.softmax(queries, dim=1)\n",
    "        \n",
    "        context = key @ values.transpose(1, 2)\n",
    "        attention = (\n",
    "            context.transpose(1, 2) @ query\n",
    "        ).reshape(n, self.key_channels, h, w)\n",
    "\n",
    "        reprojected_value = self.reprojection(attention)\n",
    "        attention = reprojected_value + value_\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetGeneratorBilinear(nn.Module):\n",
    "    def __init__(self, norm_layer):\n",
    "        super(UnetGeneratorBilinear, self).__init__()\n",
    "\n",
    "        use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        \n",
    "        self.normalize = True\n",
    "        self.self_attention = True\n",
    "        self.use_avgpool = True\n",
    "        self.skip = 0.8\n",
    "        self.use_tanh = True\n",
    "        # if self.use_tanh:\n",
    "        #     if opt.hardtanh:\n",
    "        self.final_tanh = nn.Hardtanh() \n",
    "            # else:\n",
    "            #     self.final_tanh = nn.Tanh() \n",
    "\n",
    "        p = 1\n",
    "        if self.self_attention:\n",
    "            self.conv1_1 = nn.Conv2d(6, 32, 3, padding=p)\n",
    "            self.attention_in = EfficientAttention(val_channels=3, key_channels=3, in_channels=3)\n",
    "            self.attention_out = EfficientAttention(val_channels=3, key_channels=3, in_channels=3)\n",
    "            self.attention_1 = EfficientAttention(val_channels=32, key_channels=4, in_channels=3)\n",
    "            self.attention_2 = EfficientAttention(val_channels=64, key_channels=4, in_channels=3)\n",
    "            self.attention_3 = EfficientAttention(val_channels=128, key_channels=8, in_channels=3)\n",
    "            self.attention_4 = EfficientAttention(val_channels=512, key_channels=16, in_channels=3)\n",
    "        else:\n",
    "            self.conv1_1 = nn.Conv2d(3, 32, 3, padding=p)\n",
    "\n",
    "        self.LReLU1_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn1_1 = norm_layer(32)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, 3, padding=p)\n",
    "        self.LReLU1_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn1_2 = norm_layer(32)\n",
    "        self.max_pool1 = nn.AvgPool2d(2) if self.use_avgpool == 1 else nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, 3, padding=p)\n",
    "        self.LReLU2_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn2_1 = norm_layer(64)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, 3, padding=p)\n",
    "        self.LReLU2_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn2_2 = norm_layer(64)\n",
    "        self.max_pool2 = nn.AvgPool2d(2) if self.use_avgpool == 1 else nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, 3, padding=p)\n",
    "        self.LReLU3_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn3_1 = norm_layer(128)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, 3, padding=p)\n",
    "        self.LReLU3_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn3_2 = norm_layer(128)\n",
    "        self.max_pool3 = nn.AvgPool2d(2) if self.use_avgpool == 1 else nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv4_11 = nn.Conv2d(128, 128, 1, padding=p*0)\n",
    "        self.LReLU4_11 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn4_11 = norm_layer(128)\n",
    "        self.conv4_12 = nn.Conv2d(128, 128, 3, padding=p*1)\n",
    "        self.LReLU4_12 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn4_12 = norm_layer(128)\n",
    "        self.conv4_13 = nn.Conv2d(128, 128, 5, padding=p*2)\n",
    "        self.LReLU4_13 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn4_13 = norm_layer(128)\n",
    "        self.conv4_14 = nn.Conv2d(128, 128, 7, padding=p*3)\n",
    "        self.LReLU4_14 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn4_14 = norm_layer(128)\n",
    "        self.conv4_2 = nn.Conv2d(512, 256, 3, padding=p)\n",
    "        self.LReLU4_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn4_2 = norm_layer(256)\n",
    "        \n",
    "        \n",
    "        # Uncomment this block for further downsampling\n",
    "        '''\n",
    "        self.max_pool4 = nn.AvgPool2d(2) if self.use_avgpool == 1 else nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, 3, padding=p)\n",
    "        self.LReLU5_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn5_1 = norm_layer(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=p)\n",
    "        self.LReLU5_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn5_2 = norm_layer(512)\n",
    "\n",
    "        \n",
    "        self.deconv5 = nn.Conv2d(512, 256, 3, padding=p)\n",
    "        self.conv6_1 = nn.Conv2d(512, 256, 3, padding=p)\n",
    "        self.LReLU6_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn6_1 = norm_layer(256)\n",
    "        self.conv6_2 = nn.Conv2d(256, 256, 3, padding=p)\n",
    "        self.LReLU6_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn6_2 = norm_layer(256)\n",
    "        '''\n",
    "\n",
    "        self.deconv6 = nn.Conv2d(256, 128, 3, padding=p)\n",
    "        self.conv7_1 = nn.Conv2d(256, 128, 3, padding=p)\n",
    "        self.LReLU7_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn7_1 = norm_layer(128)\n",
    "        self.conv7_2 = nn.Conv2d(128, 128, 3, padding=p)\n",
    "        self.LReLU7_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn7_2 = norm_layer(128)\n",
    "\n",
    "        self.deconv7 = nn.Conv2d(128, 64, 3, padding=p)\n",
    "        self.conv8_1 = nn.Conv2d(128, 64, 3, padding=p)\n",
    "        self.LReLU8_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn8_1 = norm_layer(64)\n",
    "        self.conv8_2 = nn.Conv2d(64, 64, 3, padding=p)\n",
    "        self.LReLU8_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn8_2 = norm_layer(64)\n",
    "\n",
    "        self.deconv8 = nn.Conv2d(64, 32, 3, padding=p)\n",
    "        self.conv9_1 = nn.Conv2d(64, 32, 3, padding=p)\n",
    "        self.LReLU9_1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.bn9_1 = norm_layer(32)\n",
    "        self.conv9_2 = nn.Conv2d(32, 32, 3, padding=p)\n",
    "        self.LReLU9_2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(32, 3, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.self_attention:\n",
    "            attended_inp = self.attention_in(input)\n",
    "            x = self.bn1_1(self.LReLU1_1(self.conv1_1(torch.cat([input, attended_inp], dim=1))))\n",
    "        else:\n",
    "            x = self.bn1_1(self.LReLU1_1(self.conv1_1(input)))\n",
    "        conv1 = self.bn1_2(self.LReLU1_2(self.conv1_2(x)))\n",
    "        x = self.max_pool1(conv1)\n",
    "\n",
    "        x = self.bn2_1(self.LReLU2_1(self.conv2_1(x)))\n",
    "        conv2 = self.bn2_2(self.LReLU2_2(self.conv2_2(x)))\n",
    "        x = self.max_pool2(conv2)\n",
    "\n",
    "        x = self.bn3_1(self.LReLU3_1(self.conv3_1(x)))\n",
    "        conv3 = self.bn3_2(self.LReLU3_2(self.conv3_2(x)))\n",
    "        x = self.max_pool3(conv3)\n",
    "\n",
    "        ########## Starts: Bottom of the U-NET ##########\n",
    "        x_1 = self.bn4_11(self.LReLU4_11(self.conv4_11(x)))\n",
    "        x_2 = self.bn4_12(self.LReLU4_12(self.conv4_12(x)))\n",
    "        x_3 = self.bn4_13(self.LReLU4_13(self.conv4_13(x)))\n",
    "        x_4 = self.bn4_14(self.LReLU4_14(self.conv4_14(x)))\n",
    "        x = torch.cat([x_1,x_2,x_3,x_4], dim=1)\n",
    "        x = self.attention_4(x, input) if self.self_attention else x\n",
    "        conv6 = self.bn4_2(self.LReLU4_2(self.conv4_2(x)))\n",
    "        \n",
    "        # uncomment this block for further downsampling\n",
    "        '''\n",
    "        x = self.bn4_1(self.LReLU4_1(self.conv4_1(x)))\n",
    "        conv4 = self.bn4_2(self.LReLU4_2(self.conv4_2(x)))\n",
    "        x = self.max_pool4(conv4)\n",
    "\n",
    "        x = self.bn5_1(self.LReLU5_1(self.conv5_1(x)))\n",
    "        #x = x*attention_map5 if self.self_attention else x\n",
    "        x = self.attention_5(x) if self.self_attention else x\n",
    "        conv5 = self.bn5_2(self.LReLU5_2(self.conv5_2(x)))\n",
    "        \n",
    "        conv5 = F_upsample(conv5, scale_factor=2, mode='bilinear')\n",
    "        #conv4 = conv4*attention_map4 if self.self_attention else conv4\n",
    "        conv4 = self.attention_4(conv4) if self.self_attention else conv4\n",
    "        up6 = torch.cat([self.deconv5(conv5), conv4], 1)\n",
    "        x = self.bn6_1(self.LReLU6_1(self.conv6_1(up6)))\n",
    "        conv6 = self.bn6_2(self.LReLU6_2(self.conv6_2(x)))\n",
    "        '''\n",
    "        ########### Ends: Bottom of the U-NET ##########\n",
    "\n",
    "        conv6 = F_upsample(conv6, scale_factor=2, mode='bilinear')\n",
    "        conv3 = self.attention_3(conv3, input) if self.self_attention else conv3\n",
    "        up7 = torch.cat([self.deconv6(conv6), conv3], 1)\n",
    "        x = self.bn7_1(self.LReLU7_1(self.conv7_1(up7)))\n",
    "        conv7 = self.bn7_2(self.LReLU7_2(self.conv7_2(x)))\n",
    "\n",
    "        conv7 = F_upsample(conv7, scale_factor=2, mode='bilinear')\n",
    "        conv2 = self.attention_2(conv2, input) if self.self_attention else conv2\n",
    "        up8 = torch.cat([self.deconv7(conv7), conv2], 1)\n",
    "        x = self.bn8_1(self.LReLU8_1(self.conv8_1(up8)))\n",
    "        conv8 = self.bn8_2(self.LReLU8_2(self.conv8_2(x)))\n",
    "\n",
    "        conv8 = F_upsample(conv8, scale_factor=2, mode='bilinear')\n",
    "        conv1 = self.attention_1(conv1, input) if self.self_attention else conv1\n",
    "        up9 = torch.cat([self.deconv8(conv8), conv1], 1)\n",
    "        x = self.bn9_1(self.LReLU9_1(self.conv9_1(up9)))\n",
    "        conv9 = self.LReLU9_2(self.conv9_2(x))\n",
    "\n",
    "        latent = self.conv10(conv9)\n",
    "        latent = self.attention_out(latent, input) if self.self_attention else latent\n",
    "\n",
    "        if self.skip:\n",
    "            if self.normalize:\n",
    "                min_latent = torch.amin(latent, dim=(0,2,3), keepdim=True)\n",
    "                max_latent = torch.amax(latent, dim=(0,2,3), keepdim=True)\n",
    "                latent = (latent - min_latent) / (max_latent - min_latent)\n",
    "                \n",
    "            output = latent + self.skip * input\n",
    "        else:\n",
    "            output = latent\n",
    "        \n",
    "        if self.use_tanh:\n",
    "            output = self.final_tanh(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [[\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "\n",
    "        sequence += [[\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]]\n",
    "\n",
    "        sequence += [[nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]]  # output 1 channel prediction map\n",
    "\n",
    "        for n in range(len(sequence)):\n",
    "            setattr(self, 'model' + str(n), nn.Sequential(*sequence[n]))\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        if return_features:\n",
    "            feats = [x]\n",
    "            for n in range(self.n_layers + 2):\n",
    "                feats.append(getattr(self, 'model' + str(n))(feats[-1]))    \n",
    "            return feats[1:]\n",
    "        else:\n",
    "            for n in range(self.n_layers + 2):\n",
    "                x = getattr(self, 'model' + str(n))(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SPLoss, self).__init__()\n",
    "        \n",
    "\n",
    "    def __call__(self, input, reference):\n",
    "        a = torch.sum(torch.sum(F.normalize(input, p=2, dim=2) * F.normalize(reference, p=2, dim=2),dim=2, keepdim=True))\n",
    "        b = torch.sum(torch.sum(F.normalize(input, p=2, dim=3) * F.normalize(reference, p=2, dim=3),dim=3, keepdim=True))\n",
    "        return -(a + b) / input.size(2)\n",
    "\n",
    "class GPLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GPLoss, self).__init__()\n",
    "        self.trace = SPLoss()\n",
    "  \n",
    "    def get_image_gradients(self,input):        \n",
    "        f_v_1 = F.pad(input,(0,-1,0,0))\n",
    "        f_v_2 = F.pad(input,(-1,0,0,0))\n",
    "        f_v = f_v_1-f_v_2\n",
    "\n",
    "        f_h_1 = F.pad(input,(0,0,0,-1))\n",
    "        f_h_2 = F.pad(input,(0,0,-1,0))\n",
    "        f_h = f_h_1-f_h_2\n",
    "\n",
    "        return f_v, f_h\n",
    "\n",
    "    def __call__(self, input, reference, normalize=True):\n",
    "        if normalize:\n",
    "            input = (input+1)/2\n",
    "            reference = (reference+1)/2\n",
    "\n",
    "        input_v,input_h = self.get_image_gradients(input)\n",
    "        ref_v, ref_h = self.get_image_gradients(reference)\n",
    "\n",
    "        trace_v = self.trace(input_v,ref_v)\n",
    "        trace_h = self.trace(input_h,ref_h)\n",
    "        return trace_v + trace_h\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Define different GAN objectives.\n",
    "\n",
    "    The GANLoss class abstracts away the need to create the target label tensor\n",
    "    that has the same size as the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
    "        \"\"\" Initialize the GANLoss class.\n",
    "\n",
    "        Parameters:\n",
    "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
    "            target_real_label (bool) - - label for a real image\n",
    "            target_fake_label (bool) - - label of a fake image\n",
    "\n",
    "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
    "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode in ['wgangp']:\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def MSE_loss_weighted(self, prediction, target, mask):\n",
    "        return (mask * ((prediction - target)**2)).mean()\n",
    "\n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        \"\"\"Create label tensors with the same size as the input.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            A label tensor filled with ground truth label, and with the size of the input\n",
    "        \"\"\"\n",
    "\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real, mask=None):\n",
    "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            the calculated loss.\n",
    "        \"\"\"\n",
    "        ''' \n",
    "        if isinstance(prediction, list):\n",
    "            if isinstance(prediction[0], list):\n",
    "                loss = 0\n",
    "                for pred in prediction:\n",
    "                    loss += self.calculate_loss(pred[-1], target_is_real)\n",
    "                return loss\n",
    "            else:\n",
    "                return self.calculate_loss(prediction[-1], target_is_real)\n",
    "        else:    \n",
    "        '''\n",
    "        return self.calculate_loss(prediction, target_is_real, mask)\n",
    "    \n",
    "    def calculate_loss(self, prediction, target_is_real, mask):\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            if self.gan_mode == 'lsgan' and mask is not None:\n",
    "                mask = F_upsample(mask, size=prediction.shape[-2:])\n",
    "                loss = self.MSE_loss_weighted(prediction, target_tensor, mask)\n",
    "            else:\n",
    "                loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss\n",
    "    \n",
    "def get_optimizer(model,lr, extra_model=None):\n",
    "    \"\"\"Return an optimizer for the model\n",
    "\n",
    "    Parameters:\n",
    "        model               -- model that whose parameters will be optimized \n",
    "        opt (option class)  -- stores all the experiment flags; needs to be a subclass of BaseOptions．\n",
    "                               opt.optim_[model_name] is the name of optimizer: SGD | Adam.　\n",
    "        model_name          -- name of the model, needed for fetching the correct values for the model from opt.　\n",
    "    \"\"\"\n",
    "\n",
    "    learnable_params = list(model.parameters())\n",
    "    if extra_model is not None:\n",
    "        learnable_params += extra_model.parameters()\n",
    "\n",
    "    return torch.optim.Adam(learnable_params, lr=lr, betas=(0.5, 0.999))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulConstant(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ctx.constant = constant\n",
    "        return tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input = ctx.constant * grad_input\n",
    "        return grad_input, None\n",
    "    \n",
    "class CustomNormalization(nn.Module):\n",
    "    def __init__(self, normalize):\n",
    "        super(CustomNormalization, self).__init__()\n",
    "        self.normalize = normalize\n",
    "        \n",
    "\n",
    "    def forward(self, x, alpha=1.0):\n",
    "        \n",
    "            # this function acts as an identity in the forward pass,\n",
    "            # but scales the gradients in the backward pass.\n",
    "        x = MulConstant.apply(x, alpha)\n",
    "\n",
    "        if self.normalize: \n",
    "            # map the tanh output to 8bit range\n",
    "            x = x + 1\n",
    "            x = x / 2 * 255\n",
    "            # apply the normalization values for the pretrained detection network\n",
    "            x = F_normalize(x, \n",
    "                    [123.675, 116.28, 103.53], \n",
    "                    [58.395, 57.12, 57.375]\n",
    "            )\n",
    "        return x\n",
    "custom_normalization=CustomNormalization(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ee/Thesis/traffic_pred/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ee/Thesis/traffic_pred/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:{}'.format(torch.cuda.current_device()))      \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "lambda_gpl=0.8\n",
    "lambda_feat=10\n",
    "lambda_gr=0\n",
    "gan_mode='lsgan'\n",
    "norm_layer='instance'\n",
    "lr_Det=5e-4\n",
    "lr_D=2e-4\n",
    "lr_G=2e-4\n",
    "beta1=0.5\n",
    "lr_decay_epoch_Det=30\n",
    "lr_decay_epoch_D=20\n",
    "lr_decay_epoch_G=20\n",
    "n_layers_D=3\n",
    "n_epochs_model=20\n",
    "n_epochs_decay_G=30 \n",
    "n_epochs_decay_D=30 \n",
    "unroll=0\n",
    "lr_policy_D='linear'\n",
    "alpha_disc=1.0\n",
    "alpha_det=1.0\n",
    "alpha_mode_det='cg_id'\n",
    "alpha_epochs_det=5\n",
    "alpha_mode_disc='Det'\n",
    "maximize_detection_loss=True  #generator tries to maximize the detection loss\n",
    "simult_det_update=False #NOT USED. if set, update the generator and detector simultaneouslys\n",
    "input_nc=3\n",
    "output_nc=3\n",
    "feature_matching=True #use feature matching loss on the discriminator\n",
    "lr_start_epoch_Det=0\n",
    "with_detector=True\n",
    "train_on_real=True #train the detector on real images as well\n",
    "freeze_detector=False #If set, detector network is not updated\n",
    "no_disc_loss=False #If set, do not use discriminator loss (only for debugging)\n",
    "#input_nc the number of channels in input images\n",
    "#ndf the number of filters in the first conv layer\n",
    "norm_layer1 = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "netG = UnetGeneratorBilinear(norm_layer=norm_layer).to(device)\n",
    "netD = NLayerDiscriminator(input_nc=input_nc+output_nc,ndf=64,n_layers=3,norm_layer=norm_layer1).to(device)\n",
    "#https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\n",
    "netDet=retinanet_resnet50_fpn(pretrained=True)\n",
    "in_features=netDet.head.classification_head.conv[0].out_channels\n",
    "netDet.head.classification_head=torchvision.models.detection.retinanet.RetinaNetClassificationHead(\n",
    "   in_channels=in_features,\n",
    "    num_anchors=netDet.head.classification_head.num_anchors,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "netDet=netDet.to(device)\n",
    "criterionGAN = GANLoss(gan_mode).to(device)\n",
    "criterionFeat = nn.L1Loss().to(device)\n",
    "citerionGPL = GPLoss().to(device) \n",
    "\n",
    "\n",
    "optimizer_D = torch.optim.Adam(netD.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
    "optimizer_G = torch.optim.Adam(netG.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
    "optimizer_Det = torch.optim.Adam(netDet.parameters(), lr=lr_Det, betas=(0.5, 0.999))\n",
    "#scheduler, backprop, detector, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad2(disc_out, real_data):\n",
    "    ''' Calculate the gradient norm for regularization. Different from WGAN-GP, norm is zero centered.\n",
    "\n",
    "    Arguments:\n",
    "        disc_out (tensor)           -- discriminator output\n",
    "        real_data (tensor array)    -- real images\n",
    "\n",
    "    Returns the squared gradient norm     \n",
    "    '''\n",
    "    batch_size = real_data.size(0)\n",
    "    grad_dout = torch.autograd.grad(\n",
    "        outputs=disc_out.sum(), inputs=real_data,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    grad_dout2 = grad_dout.pow(2)\n",
    "    \n",
    "    assert(grad_dout2.size() == real_data.size())\n",
    "    \n",
    "    reg = grad_dout2.view(batch_size, -1).sum(1)\n",
    "    return reg\n",
    "\n",
    "def detect_objects(detach_input, current_iter, detect_real,real_B,fake_B,bboxes,labels):\n",
    "        # print(real_B,fake_B)\n",
    "        isTrain=True\n",
    "        netDet.train()\n",
    "        targets=[]\n",
    "        if current_iter:\n",
    "            if 'ig' in alpha_mode_det:\n",
    "                alpha_weight = current_iter / (alpha_epochs_det*dataset_size)\n",
    "                curr_alpha = float(min(alpha_det, alpha_det * alpha_weight))\n",
    "            else:\n",
    "                curr_alpha = float(alpha_det)\n",
    "        else:\n",
    "            curr_alpha = 1.0\n",
    "\n",
    "        if detect_real:\n",
    "            _to_detector = custom_normalization(real_B, alpha=curr_alpha)\n",
    "        else:\n",
    "            if isTrain and maximize_detection_loss:\n",
    "                curr_alpha = -1 * curr_alpha\n",
    "            _to_detector = custom_normalization(fake_B, alpha=curr_alpha)\n",
    "        \n",
    "        if detach_input:\n",
    "            _to_detector = _to_detector.detach()\n",
    "        # print(_to_detector.shape)\n",
    "        # for bs in range(_to_detector.shape[0]):\n",
    "        targets=[{'boxes': bboxes, 'labels': labels}]\n",
    "        # print(len(targets),targets[0]['boxes'].shape,targets[0]['labels'].shape)\n",
    "        # for bbox, cls_labels in zip(bboxes, labels)]\n",
    "        detector_out = netDet(_to_detector,targets)\n",
    "        if detect_real:\n",
    "            training_losses['classification_real_loss'].append(detector_out['classification'])\n",
    "            training_losses['regression_real_loss'].append(detector_out['bbox_regression'])\n",
    "        else:\n",
    "            training_losses['classification_fake_loss'].append(detector_out['classification'])\n",
    "            training_losses['regression_fake_loss'].append(detector_out['bbox_regression'])\n",
    "            \n",
    "        # print(detector_out)\n",
    "        return detector_out\n",
    "    \n",
    "def parse_detection_loss(register_loss,detector_out):\n",
    "        \n",
    "        # losses={}\n",
    "        # losses['classification_loss'] = class_loss_func(detector_out[0]['scores'].view(-1, num_classes), cls_labels.view(-1))\n",
    "        # losses['bbox_loss'] = l1_loss_func(detector_out[0]['boxes'], bboxes_true)\n",
    "        # log_vars = {}\n",
    "        # for loss_name, loss_value in losses.items():\n",
    "        #     log_vars[loss_name] = loss_value.mean()  # Calculating mean might be redundant depending on tensor shape\n",
    "        # loss = sum(value for key, value in log_vars.items() if 'loss' in key)\n",
    "        \n",
    "        loss=detector_out['classification']+detector_out['bbox_regression']\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "def _update_D(current_epoch):\n",
    "        '''returns true if the discriminator needs to be updated'''\n",
    "        return (not no_disc_loss and\n",
    "                current_epoch <= n_epochs_model)\n",
    "def _update_Det(current_epoch):\n",
    "        '''returns true if the detector needs to be updated'''\n",
    "        return  (with_detector and not freeze_detector and \n",
    "                current_epoch <= n_epochs_model and \n",
    "                lr_start_epoch_Det <= current_epoch)\n",
    "def _update_G(current_epoch):\n",
    "        '''returns true if the generator needs to be updated'''\n",
    "        return current_epoch <= n_epochs_model\n",
    "def backward_D(real_A,fake_B,real_B):\n",
    "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
    "        optimizer_D.zero_grad() \n",
    "        features = None\n",
    "        # Fake; stop backprop to the generator by detaching fake_B\n",
    "        fake_AB = torch.cat((real_A, fake_B), 1) \n",
    "        pred_fake = netD(fake_AB.detach())\n",
    "        loss_D_fake = criterionGAN(pred_fake, False)\n",
    "        loss_D_fake.backward()\n",
    "\n",
    "        # Real\n",
    "        real_AB = torch.cat((real_A, real_B), 1).requires_grad_()\n",
    "        pred_real = netD(real_AB,feature_matching)\n",
    "        loss_D_real = criterionGAN(pred_real[-1] if feature_matching else pred_real, True, mask=None)\n",
    "        # regularize the discriminator with gradient norm.\n",
    "        if lambda_gr:\n",
    "            loss_D_real.backward(retain_graph=True)\n",
    "            grad_reg = lambda_gr * compute_grad2(pred_real, real_AB).mean()\n",
    "            grad_reg.backward()\n",
    "        else:\n",
    "            loss_D_real.backward()\n",
    "        \n",
    "        optimizer_D.step()\n",
    "        return features\n",
    "\n",
    "def backward_G(current_iter, current_epoch,real_A,fake_B,real_B,bbox,cls_labels):\n",
    "        \"\"\"Calculate Detection, GAN and L1 loss for the generator\"\"\"\n",
    "        \n",
    "        debug_grads = False\n",
    "        debug_grad_norms=False\n",
    "        # if debug_grad_norms:\n",
    "        #     grads_iter_modulo = current_iter % 400\n",
    "        #     number_of_samples = number_of_iters * batch_size\n",
    "        #     debug_grads = debug_grad_norms and grads_iter_modulo <= number_of_samples \n",
    "        #     is_last = grads_iter_modulo == (number_of_samples)\n",
    "\n",
    "        if _update_G(current_epoch):\n",
    "        \n",
    "            optimizer_G.zero_grad()\n",
    "            ###############################################################################\n",
    "            # #TODO: Decrease the contribution from all discriminator-related losses      # \n",
    "            # proportionally to the decay of the learning rate of discriminator.          #\n",
    "            # I do not do this for now; thus the weight is always 1.                      #\n",
    "            ###############################################################################\n",
    "            \n",
    "            if alpha_mode_disc == 'dec':\n",
    "                if lr_policy_D == 'step':\n",
    "                    disc_weight = 0.1 ** (current_epoch // lr_decay_epoch_D)\n",
    "                    #disc_weight = 0.1 if (current_iter // self.opt.dataset_size) >= self.opt.lr_decay_epoch_D else 1\n",
    "                else:\n",
    "                    if n_epochs_decay_D == 0:\n",
    "                        disc_weight = 1\n",
    "                    else:\n",
    "                        disc_weight = (n_epochs_model*dataset_size - current_iter)/(n_epochs_decay_D*dataset_size)\n",
    "                disc_weight = min(alpha_disc, alpha_disc*disc_weight)\n",
    "            else:\n",
    "                disc_weight = alpha_disc\n",
    "            \n",
    "            use_disc_loss = _update_D(current_epoch)\n",
    "\n",
    "            # overall loss\n",
    "            loss_G = 0\n",
    "\n",
    "            # GAN-related losses\n",
    "            loss_G_GAN = 0\n",
    "            loss_G_GAN_feat = 0\n",
    "            # gradient profile loss (cosine similarity between gradient/edge maps)\n",
    "            loss_G_GPL = 0.\n",
    "            if disc_weight > 0 and use_disc_loss:\n",
    "                # G(A) should fake the discriminator\n",
    "                fake_AB = torch.cat((real_A, fake_B), 1)\n",
    "                pred_fake = netD(fake_AB, feature_matching)\n",
    "                loss_G_GAN = criterionGAN(pred_fake[-1] if feature_matching else pred_fake, True, mask=None)\n",
    "                training_losses['gan_loss'].append(loss_G_GAN)\n",
    "                loss_G = loss_G + loss_G_GAN * disc_weight\n",
    "                # feature matching loss\n",
    "                # match the features of fake and real in the intermediate layers of the discriminator\n",
    "                if feature_matching:\n",
    "                    real_AB = torch.cat((real_A, real_B), 1)\n",
    "                    pred_real = netD(real_AB, return_features=True)\n",
    "                    feat_weights = 4.0 / (n_layers_D + 1)\n",
    "                    for i in range(len(pred_fake)-1):\n",
    "                        loss_G_GAN_feat += feat_weights * criterionFeat(pred_fake[i], \n",
    "                                                                            pred_real[i].detach()) * lambda_feat\n",
    "                        training_losses['feat_loss'].append(loss_G_GAN_feat)\n",
    "                    loss_G = loss_G + loss_G_GAN_feat * disc_weight\n",
    "                \n",
    "                if lambda_gpl:\n",
    "                    #TODO: can use either real LDR or HDR as a reference. These options should be compared.\n",
    "                    loss_G_GPL = citerionGPL(fake_B, real_B, normalize=True)\n",
    "                    training_losses['gpl_loss'].append(loss_G_GPL)\n",
    "                    loss_G = loss_G + loss_G_GPL * lambda_gpl\n",
    "\n",
    "            # Det(G(A)) should detect objects.\n",
    "            if with_detector:    \n",
    "                if unroll > 0:\n",
    "                    # see how detector reacts to real images + fake images \n",
    "                    # by unrolling <unroll> many steps\n",
    "                    backup = copy.deepcopy(netDet.module.state_dict())\n",
    "                    # for _ in range(unroll):\n",
    "                    #     backward_Det_unrolled(current_iter, current_epoch)\n",
    "\n",
    "                # detect objects on generated images to update the generator\n",
    "                optimizer_Det.zero_grad()\n",
    "                # print(bbox.shape)\n",
    "                 #some issue over here\n",
    "                # detector_out=detect_objects(detach_input=False, current_iter=current_iter, detect_real=False,real_B=real_B,fake_B=fake_B,labels=cls_labels,bboxes=bbox)\n",
    "                # detector_loss = parse_detection_loss(register_loss=simult_det_update,detector_out=detector_out)\n",
    "                # loss_G = loss_G + detector_loss\n",
    "            \n",
    "            if not debug_grads:\n",
    "                loss_G.backward()  \n",
    "                optimizer_G.step()\n",
    "\n",
    "            if  unroll > 0:\n",
    "                netDet.module.load_state_dict(backup)    \n",
    "                del backup\n",
    "        \n",
    "        if not debug_grads:\n",
    "            if _update_Det(current_epoch) and simult_det_update:\n",
    "                optimizer_Det.step()\n",
    "\n",
    "def backward_Det(current_iter, current_epoch,real_B,fake_B,cls_labels,bbox):\n",
    "        \n",
    "\n",
    "        optimizer_Det.zero_grad()     \n",
    "\n",
    "        detector_out=detect_objects(detach_input=True, current_iter=current_iter, detect_real=False,real_B=real_B,fake_B=fake_B,bboxes=bbox,labels=cls_labels)\n",
    "        detector_loss = parse_detection_loss(register_loss=not maximize_detection_loss,detector_out=detector_out)\n",
    "        \n",
    "       \n",
    "        \n",
    "        if train_on_real:\n",
    "            # print(bbox.shape)\n",
    "            detector_out=detect_objects(detach_input=False,current_iter=current_iter, detect_real=True,real_B=real_B,fake_B=fake_B,labels=cls_labels,bboxes=bbox)\n",
    "            detector_loss_real = parse_detection_loss(register_loss=maximize_detection_loss, detector_out=detector_out)\n",
    "            detector_loss = detector_loss + detector_loss_real\n",
    "        \n",
    "        detector_loss.backward()\n",
    "        optimizer_Det.step()\n",
    "        \n",
    "\n",
    "def optimize_parameters(current_iter, current_epoch,real_A,real_B,labels,bboxes):\n",
    "    \n",
    "        # compute fake images: G(A)\n",
    "        fake_B = netG(real_A) #forward\n",
    "\n",
    "        # update D\n",
    "        backward_D(real_A,fake_B,real_B)\n",
    "\n",
    "\n",
    "        # update Det\n",
    "        if not simult_det_update:\n",
    "            # print(bboxes.shape)\n",
    "            backward_Det(current_iter, current_epoch,real_B,fake_B,labels,bboxes)\n",
    "        # print(bboxes.shape)\n",
    "        # update G\n",
    "        backward_G(current_iter, current_epoch,real_A,fake_B,real_B,labels,bboxes)\n",
    "        \n",
    "def hdr_normalize(img):\n",
    "    hdr_max=65830.18848\n",
    "    hdr_min=-326.18848\n",
    "    real_A = F_normalize(img, \n",
    "                        [hdr_min, hdr_min, hdr_min],\n",
    "                        [hdr_max, hdr_max, hdr_max]\n",
    "     )\n",
    "     \n",
    "    return real_A\n",
    "\n",
    "def evaluate(loader,):\n",
    "     test_ssim=[]\n",
    "     with torch.no_grad():\n",
    "         for i, data in enumerate(loader):\n",
    "            real_A,real_B, labels, bboxes =data\n",
    "       \n",
    "            if len(bboxes.shape)>=3:\n",
    "                # print(bboxes.shape)\n",
    "                bboxes=bboxes.squeeze(0)\n",
    "                labels=labels.squeeze(0)\n",
    "                # print(labels.shape,labels,bboxes.shape,bboxes)\n",
    "                real_A=hdr_normalize(real_A)\n",
    "                fake_B=netG(real_A)\n",
    "                netDet.eval()\n",
    "                detector_out=netDet(fake_B)\n",
    "                '''\n",
    "                boxes:\n",
    "                labels:\n",
    "                scores:\n",
    "                '''\n",
    "                # print(detector_out)    \n",
    "                test_ssim.append(ssim(real_A,fake_B))\n",
    "            return np.array(test_ssim).mean()\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_val=10000.0\n",
    "best_gan_model = \"best_gan_model.pt\"\n",
    "best_detector_model = \"best_detector_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m training_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgan_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     17\u001b[0m training_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     19\u001b[0m     real_A,real_B, labels, bboxes \u001b[38;5;241m=\u001b[39mdata\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# print(real_A.shape,real_B.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 69\u001b[0m, in \u001b[0;36mHDRDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Convert image and bbox to tensor\u001b[39;00m\n\u001b[1;32m     68\u001b[0m png_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(png_image, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Channel first format\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m png_image\u001b[38;5;241m=\u001b[39m\u001b[43mtransform_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpng_image\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# print('png_image:',png_image)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m exr_image\u001b[38;5;241m=\u001b[39mtransform_img(exr_image)\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torchvision/transforms/functional.py:492\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py:467\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m~/Thesis/traffic_pred/lib/python3.8/site-packages/torch/nn/functional.py:4020\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4014\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[1;32m   4015\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[1;32m   4016\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4017\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4018\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mupsample_bilinear2d_vec(\n\u001b[1;32m   4019\u001b[0m                 \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 4020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bilinear2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4022\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs=30\n",
    "epoch_ssim=[]\n",
    "epoch_training_losses={}\n",
    "epoch_training_losses['classification_real_loss']=[]\n",
    "epoch_training_losses['classification_fake_loss']=[]\n",
    "epoch_training_losses['regression_real_loss']=[]\n",
    "epoch_training_losses['regression_fake_loss']=[]\n",
    "epoch_training_losses['gpl_loss']=[]\n",
    "epoch_training_losses['gan_loss']=[]\n",
    "epoch_training_losses['feat_loss']=[]\n",
    "for epoch in range(n_epochs):\n",
    "    training_losses['classification_real_loss']=[]\n",
    "    training_losses['classification_fake_loss']=[]\n",
    "    training_losses['regression_real_loss']=[]\n",
    "    training_losses['regression_fake_loss']=[]\n",
    "    training_losses['gpl_loss']=[]\n",
    "    training_losses['gan_loss']=[]\n",
    "    training_losses['feat_loss']=[]\n",
    "    for i, data in enumerate(train_loader):\n",
    "        real_A,real_B, labels, bboxes =data\n",
    "        # print(real_A.shape,real_B.shape)\n",
    "        if len(bboxes.shape)>=3:\n",
    "            bboxes=bboxes.squeeze(0)\n",
    "            labels=labels.squeeze(0)\n",
    "            real_A=hdr_normalize(real_A.to(device))\n",
    "            optimize_parameters(epoch*dataset_size,epoch,real_A,real_B.to(device),labels.to(device),bboxes.to(device))\n",
    "    epoch_training_losses['classification_real_loss'].append(np.array(training_losses['classification_real_loss']).mean())\n",
    "    epoch_training_losses['classification_fake_loss'].append(np.array(training_losses['classification_fake_loss']).mean())\n",
    "    epoch_training_losses['regression_real_loss'].append(np.array(training_losses['regression_real_loss']).mean())\n",
    "    epoch_training_losses['regression_fake_loss'].append(np.array(training_losses['regression_fake_loss']).mean())\n",
    "    epoch_training_losses['gpl_loss'].append(np.array(training_losses['gpl_loss']).mean())\n",
    "    epoch_training_losses['gan_loss'].append(np.array(training_losses['gan_loss']).mean())\n",
    "    epoch_training_losses['feat_loss'].append(np.array(training_losses['feat_loss']).mean())\n",
    "    loss_val=np.array(training_losses['classification_fake_loss']).mean()\n",
    "    if loss_val< best_loss_val:\n",
    "        torch.save(netG.state_dict(), best_gan_model)\n",
    "        torch.save(netDet.state_dict(), best_detector_model)\n",
    "        best_loss_val=loss_val\n",
    "        \n",
    "    _ssim=evaluate(test_loader)\n",
    "    epoch_ssim.append(_ssim)\n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
