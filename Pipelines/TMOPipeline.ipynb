{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "SZXN0o2DXDOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk1EVZp7Vx1-"
      },
      "outputs": [],
      "source": [
        "# -------------------- IGNORE THIS BLOCK AT THE MOMENT --------------------\n",
        "\n",
        "\n",
        "# Assuming G, D, and F are your generator, discriminator, and detector models\n",
        "\n",
        "# Detector loss components (simplified)\n",
        "class DetectorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DetectorLoss, self).__init__()\n",
        "        # Define loss components, e.g., Focal Loss for classification\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # Compute classification and localization loss\n",
        "        # Lcls and Lloc from Eq. (5)\n",
        "        return lcls + lloc\n",
        "\n",
        "# Generator and Discriminator loss components\n",
        "class GANDetLoss(nn.Module):\n",
        "    def __init__(self, alpha_det=1.0, alpha_non_det=1.0, beta=0.8, gamma=10.0):\n",
        "        super(GANDetLoss, self).__init__()\n",
        "        self.alpha_det = alpha_det\n",
        "        self.alpha_non_det = alpha_non_det\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        # Additional components, e.g., feature matching loss, could be defined here\n",
        "\n",
        "    def forward(self, hdr_images, ldr_images, ground_truth):\n",
        "        # Calculate LG, LD, and LDet as described in Eq. (4), (6), and (5)\n",
        "        # This includes calling the DetectorLoss for LDet\n",
        "        # Note: This is a simplified outline. Actual implementation will depend on how G, D, and F are defined\n",
        "        return ltmo_det\n",
        "\n",
        "# Instantiate the loss\n",
        "loss_fn = GANDetLoss()\n",
        "\n",
        "# Example forward pass (simplified)\n",
        "# hdr_images, ldr_images, ground_truth = your data loading logic here\n",
        "loss = loss_fn(hdr_images, ldr_images, ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- IGNORE THIS BLOCK AT THE MOMENT --------------------\n",
        "\n",
        "def discriminator_loss(D, real_images, fake_images):\n",
        "  real_loss = F.mse_loss(D(real_images), torch.ones_like(D(real_images)))\n",
        "  fake_loss = F.mse_loss(D(fake_images), torch.zeros_like(D(fake_images)))\n",
        "  return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(G, fake_images):\n",
        "  return F.mse_loss(G(fake_images), torch.ones_like(G(fake_images)))\n",
        "\n",
        "# Optional: L1 Content Loss to enforce similarity between generated and real LDR images\n",
        "def l1_content_loss(fake_images, real_images):\n",
        "    return torch.mean(torch.abs(fake_images - real_images))\n",
        "\n",
        "\n",
        "# def detector_loss():\n",
        "#   # TBD\n",
        "#   pass"
      ],
      "metadata": {
        "id": "Hu82CHj2WGmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- BASIC GAN LOSSES -------------------------\n",
        "\n",
        "def generator_adversarial_loss(D, fake_images):\n",
        "    return torch.mean((D(fake_images) - 1) ** 2)\n",
        "\n",
        "def discriminator_loss(D, real_images, fake_images):\n",
        "    real_loss = torch.mean((D(real_images) - 1) ** 2)\n",
        "    fake_loss = torch.mean(D(fake_images) ** 2)\n",
        "    return (real_loss + fake_loss) / 2"
      ],
      "metadata": {
        "id": "iXqnnyIMXTWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- GENERATOR ARCHITECTURE -------------------------\n",
        "\n",
        "class ConvBlock(nn.module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.ins = nn.InstanceNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.ins(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class AttentionModule(nn.module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(AttentionModule, self).__init__()\n",
        "    self.attention_score = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        score = self.attention_score(x)\n",
        "        score = self.sigmoid(score)\n",
        "        attention_map = torch.mul(score, x)\n",
        "        return attention_map\n",
        "\n",
        "class Generator(nn.module)\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.conv1 = ConvBlock(in_channels, kernel_size=1, stride=1, padding=None)\n",
        "        self.conv2 = ConvBlock(32, 64, kernel_size=1, stride=1, padding=None)\n",
        "        self.conv3 = ConvBlock(64, 128, kernel_size=1, stride=1, padding=None)\n",
        "        self.conv4 = ConvBlock(512, 128, kernel_size=1, stride=1, padding=None)\n",
        "        self.conv5 = ConvBlock(128, 64, kernel_size=1, stride=1, padding=None)\n",
        "        self.conv6 = ConvBlock(64, 3, kernel_size=1, stride=1, padding=None)\n",
        "\n",
        "        self.attention1 = AttentionModule(32, 32)\n",
        "        self.attention2 = AttentionModule(64, 64)\n",
        "        self.attention3 = AttentionModule(512, 512)\n",
        "\n",
        "        self.k3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.k5 = nn.Conv2d(128, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.k7 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3)\n",
        "        self.k9 = nn.Conv2d(128, 128, kernel_size=9, stride=1, padding=4)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest') # TESTING\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.conv1(x)\n",
        "            x = self.maxpool(x)\n",
        "            att_1 = self.attention1(x)\n",
        "            x = self.conv2(x)\n",
        "            x = self.maxpool(x)\n",
        "            att_2 = self.attention2(x)\n",
        "            x = self.conv3(x)\n",
        "            x = self.maxpool(x)\n",
        "            x_1 = self.k3(x)\n",
        "            x_2 = self.k5(x)\n",
        "            x_3 = self.k7(x)\n",
        "            x_4 = self.k9(x)\n",
        "            x = torch.cat((x_1, x_2, x_3, x_4), dim=1) # TESTING\n",
        "            att_3 = self.attention3(x)\n",
        "            x = self.upsample(att_3) # CONTINUE CODING HERE\n"
      ],
      "metadata": {
        "id": "6fq4dVtTiD6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------- DISCRIMINATOR ARCHITECTURE -------------------------\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        # Block 1 Input -> Channels x H x W\n",
        "        nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        # Block 2 Input -> 64 * H/2 * W/2\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(128),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
        "        nn.InstanceNorm2d(512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "        nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "HVTIYlw2MXpW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}